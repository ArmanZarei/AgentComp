<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">

    <meta property='og:title' content='AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models'/>
    <meta property='og:url' content='https://armanzarei.github.io/AgentComp'/>
    <meta property="og:image" content="static/images/banner.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    <title>AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
    rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/tab_gallery.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link rel="stylesheet" href="juxtapose/css/juxtapose.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/magnifier.js"></script>
    <link href="https://fonts.cdnfonts.com/css/menlo" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/image_card_fader.css">
    <link rel="stylesheet" href="./static/css/image_card_slider.css">

    <style>
.toggle-container {
    width: 100%;
    margin: 10px 0;
    background: #f5f5f5;
    padding: 5px 10px;
    border-radius: 8px;
}

.toggle-header {
  cursor: pointer;
  font-weight: bold;
  color: #777777;
  user-select: none;
}

.toggle-content {
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.35s ease-out;
}
    </style>

</head>

<body>
  <section class="hero banner">
  <div class="hero-body" style="padding-bottom: 1rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://armanzarei.github.io/">Arman Zarei</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jiacheng-pan/">Jiacheng Pan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=lB9WkQ0AAAAJ&hl=en">Matthew Gwilliam</a><sup>1</sup>,
            <span class="author-block">
              <a href="https://www.cs.umd.edu/~sfeizi/">Soheil Feizi</a><sup>2</sup>,
            </span>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Ds5wwRoAAAAJ&hl=en">Zhenheng Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <sup>1</sup><span class="author-block" style="margin-right: 25px;">TikTok</span> <sup>2</sup><span class="author-block">University of Maryland</span>
          </div>

          <div class="is-size-5 publication-venue mt-4">
            Under Review
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2512.09081"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2512.09081"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset (Coming Soon)</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-obp"></i>
                  </span>
                  <span>BibTex</span>
                </a>
              </span> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
  <div class="content has-text-centered">

    <img src="./static/images/teasor.jpg" width="100%">
    <p style="text-align: justify;">AgentComp significantly enhances the compositional abilities of text-to-image generative models, improving text–image alignment while preserving image quality and even boosting capabilities such as text rendering, despite not being explicitly trained for it.</p>
  </div>
    <div class="columns is-centered has-text-centered" style="margin-top: 30px;">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality—accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. 
A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details.
To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability.
AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality—a common drawback in prior approaches—and even generalizes to other capabilities not explicitly trained for, such as text rendering. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  
    <!--/ Matting. -->
    <div class="container is-max-desktop">
    
    <!-- Latent space editing applications -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method Overview</h2>

        <div class="content has-text-centered">
          <img src="./static/images/method.jpg">
        </div>

        <!-- Prompt Interpolation image -->
        
        <div class="content has-text-justified">
          <p>
            We propose an agentic orchestration framework in which specialized agents (with appropriate tools, such as image editing modules and VQA models for interacting with generated images and analyzing their compositional properties) collaborate to generate a positive image, synthesize contrastive prompts, produce corresponding negative images, and rank them according to compositional distance. These groups of near-identical yet compositionally contrasted samples are then used to explicitly train the model via our Agentic Preference Optimization method, enabling it to distinguish between denoising trajectories of visually similar but compositionally distinct samples. This targeted supervision substantially improves compositional reasoning and understanding in text-to-image models.
          </p>
        </div>
        <br>

        <!-- Prompt Interpolation image -->
        <h3 class="title is-4 has-text-centered">Results</h3>
        
        <div class="content has-text-centered">
          <img src="./static/images/dataset_generation_sample.jpg" width="60%">
          <p style="text-align: justify;"><b>Example from the dataset generated by the agentic orchestra.</b> The dataset includes high-quality samples, with reference image that accurately capture compositional details in the given prompt, along with negative samples created by subtly altering those details in the reference text–image pair.</p>
        </div>
        <br>
        
        <div class="content has-text-centered">
          <img src="./static/images/image_gen_agent_conversation.jpg" width="55%">
          <p style="text-align: justify;"><b>Example scenario of the Image Generation Agent.</b> The agent employs iterative reasoning and tool calls to produce a compositionally accurate image that aligns with the given prompt.</p>
        </div>
        <br>
        
        <div class="content has-text-centered">
          <img src="./static/images/image_edit_agent.jpg" width="90%">
          <p style="text-align: justify;"><b>Image Editing Agent Example Scenario.</b> Given a source image, its prompt, and a target prompt, the image-editing agent leverages editing tool and VQA to produce a correct contrastive sample. Although the editing tools may introduce unintended modifications (Steps 1 and 2), the agent detects these errors through reasoning and VQA feedback, adjusts its intermediate prompts, and ultimately generates the intended result.</p>
        </div>
        <br>
        
        <div class="content has-text-centered">
          <img src="./static/images/t2i_compbench_results.jpg" width="80%">
          <p style="text-align: center;"><b>Quantitative comparison of AgentComp against other baselines on T2I-CompBench.</b> AgentComp achieves state-of-the-art performance, demonstrating substantial improvements in compositional reasoning and understanding for T2I models.</p>
        </div>
        <br>
        
        <div class="content has-text-centered">
          <img src="./static/images/compositionality_categories_qualitative.jpg" width="90%">
          <p style="text-align: justify;"><b>Qualitative comparison across compositional categories.</b> AgentComp produces more compositionally accurate images than the base model across various categories.</p>
        </div>
        <br>
        
        <div class="content has-text-centered">
          <img src="./static/images/general_quality_text_rendering.jpg" width="50%">
          <p style="text-align: justify;"><b>General quality and text rendering comparison.</b> AgentComp preserves and even improves image generation quality while also significantly enhancing text rendering accuracy.</p>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>




<section class="section" id="BibTeX" style="padding-top: 15px !important;">
  <div class="container is-max-desktop content">
    <h2 class="title"><a id="bibtex">BibTeX</a></h2>
    <pre><code>@article{zarei2025agentcomp,
  title={AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models},
  author={Zarei, Arman and Pan, Jiacheng and Gwilliam, Matthew and Feizi, Soheil and Yang, Zhenheng},
  journal={arXiv preprint arXiv:2512.09081},
  year={2025}
}</code></pre>
  </div>
</section>



<script src="juxtapose/js/juxtapose.js"></script>


<!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"></script> -->
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/js/bootstrap.min.js"></script>    

<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script>
  document.querySelectorAll(".toggle-container").forEach(container => {
    const header = container.querySelector(".toggle-header");
    const content = container.querySelector(".toggle-content");

    header.addEventListener("click", () => {
      if (content.style.maxHeight) {
        content.style.maxHeight = null;
        header.innerHTML = "<i class=\"far fa-arrow-alt-circle-right\"></i> More details";
      } else {
        content.style.maxHeight = content.scrollHeight + "px";
        header.innerHTML = "<i class=\"far fa-arrow-alt-circle-down\"></i> Less details";
      }
    });
  });
</script>


</body>
</html>
